# Step 5: Data and AI

## Blob storage

* Object storage <https://en.wikipedia.org/wiki/Object_storage>
* Amazon S3 <https://aws.amazon.com/s3/>
* Azure Blob Storage <https://azure.microsoft.com/en-us/services/storage/blobs/>

## Data streaming and hubs

* Kafka <https://kafka.apache.org/>
* Amazon Managed Streaming for Apache Kafka (MSK) <https://aws.amazon.com/msk/>
* Azure Event Hubs <https://azure.microsoft.com/en-us/products/event-hubs/>
* Azure Service Bus <https://azure.microsoft.com/en-us/products/service-bus/>
* Azure Queue Storage <https://azure.microsoft.com/en-us/products/storage/queues/>
* Amazon Simple Queue Service <https://aws.amazon.com/sqs/>
* Amazon Simple Notification Service <https://aws.amazon.com/sns/>

## Database fundamentals

* Relational model <https://en.wikipedia.org/wiki/Relational_model>
* First order logic <https://en.wikipedia.org/wiki/First-order_logic>
* ER model <https://en.wikipedia.org/wiki/Entity%E2%80%93relationship_model>
* Ontology <https://en.wikipedia.org/wiki/Ontology_(information_science)>
* DIKW pyramid <https://en.wikipedia.org/wiki/DIKW_pyramid>
* ORM <https://en.wikipedia.org/wiki/Object%E2%80%93relational_mapping>
* Concurrency control <https://en.wikipedia.org/wiki/Concurrency_control>
  * ACID <https://en.wikipedia.org/wiki/ACID>
* ODBC <https://en.wikipedia.org/wiki/Open_Database_Connectivity>
* JDBC <https://en.wikipedia.org/wiki/Java_Database_Connectivity>

## DBMS and libraries

* PostgreSQL <https://www.postgresql.org/>
  * pgAdmin <https://www.pgadmin.org/>
  * pgBadger <https://pgbadger.darold.net/>
* MySQL <https://www.mysql.com/>
* Amazon RDS <https://aws.amazon.com/rds/>
* Azure SQL Database <https://azure.microsoft.com/en-us/products/azure-sql/database/>
  * sqlcmd utility <https://learn.microsoft.com/en-us/sql/tools/sqlcmd/sqlcmd-utility>
  * sqlpackage <https://learn.microsoft.com/en-us/sql/tools/sqlpackage/sqlpackage>
  * SSMS <https://learn.microsoft.com/en-us/sql/ssms/sql-server-management-studio-ssms>
  * DAC <https://learn.microsoft.com/en-us/sql/relational-databases/data-tier-applications/data-tier-applications>
* Beekeeper Studio <https://www.beekeeperstudio.io/>
* SQLite <https://www.sqlite.org/index.html>
  * DB Browser for SQLite <https://sqlitebrowser.org/>

## NoSQL data store

* ElasticSearch <https://www.elastic.co/products/elasticsearch>
  * Painless language <https://www.elastic.co/guide/en/elasticsearch/painless/current/index.html>
* MongoDB <https://www.mongodb.com/>
* Redis <https://redis.io/>
* etcd <https://etcd.io/>
* Amazon DynamoDB <https://aws.amazon.com/dynamodb/>
* Azure Cosmos DB <https://azure.microsoft.com/en-us/products/cosmos-db/>
* Neo4j <https://neo4j.com/>
* Amazon Neptune <https://aws.amazon.com/neptune/>

## Data visualization

* matplotlib <https://matplotlib.org/>
  * seaborn <https://seaborn.pydata.org/>
* Vega <https://vega.github.io/vega/>
* Vega-Lite <https://vega.github.io/vega-lite/>
* D3 <https://d3js.org/>

## Statistics fundamentals

* Probability theory <https://en.wikipedia.org/wiki/Probability_theory>
* Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>
* Bayes' theorem <https://en.wikipedia.org/wiki/Bayes%27_theorem>
* Regression analysis <https://en.wikipedia.org/wiki/Regression_analysis>
* Confusion matrix <https://en.wikipedia.org/wiki/Confusion_matrix>
  * ROC curve <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>
* Statistical hypothesis testing <https://en.wikipedia.org/wiki/Statistical_hypothesis_testing>
* Confounding <https://en.wikipedia.org/wiki/Confounding>
* Experimental design <https://en.wikipedia.org/wiki/Design_of_experiments>
* Cross-validation <https://en.wikipedia.org/wiki/Cross-validation_(statistics)>

## Statistics tools

* Pandas <https://pandas.pydata.org/>
* NumPy <https://numpy.org/>
* SciPy <https://scipy.org/>
* R project <https://www.r-project.org/>
* RStudio <https://posit.co/download/rstudio-desktop/>
* JASP <https://jasp-stats.org/>

## ML licecycle management

* MLFlow <https://mlflow.org/>
* KubeFlow <https://www.kubeflow.org/>

## Machine learning

* Common tools
  * Stochastic gradient descent <https://en.wikipedia.org/wiki/Stochastic_gradient_descent>
  * Dynamic programming <https://en.wikipedia.org/wiki/Dynamic_programming>
  * Sigmoid function <https://en.wikipedia.org/wiki/Sigmoid_function>
  * Softmax function <https://en.wikipedia.org/wiki/Softmax_function>
  * Early stopping <https://en.wikipedia.org/wiki/Early_stopping>
* Supervised learning <https://en.wikipedia.org/wiki/Supervised_learning>
  * Ensemble learning <https://en.wikipedia.org/wiki/Ensemble_learning>
  * Logistic regression <https://en.wikipedia.org/wiki/Logistic_regression>
  * Support vector machine <https://en.wikipedia.org/wiki/Support_vector_machine>
  * Random forest <https://en.wikipedia.org/wiki/Random_forest>
  * Artificial neural network <https://en.wikipedia.org/wiki/Artificial_neural_network>
  * ARIMA model <https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average>
* Unsupervised learning <https://en.wikipedia.org/wiki/Unsupervised_learning>
* Reinforcement learning <https://en.wikipedia.org/wiki/Reinforcement_learning>
  * Markov decision process <https://en.wikipedia.org/wiki/Markov_decision_process>
  * Multi-armed bandit <https://en.wikipedia.org/wiki/Multi-armed_bandit>
  * Value function <https://en.wikipedia.org/wiki/Value_function>

## Deep learning

* Deep Learning <https://en.wikipedia.org/wiki/Deep_learning>
  * Backpropagation <https://en.wikipedia.org/wiki/Backpropagation>
  * Autoencoder <https://en.wikipedia.org/wiki/Autoencoder>
  * Vanishing gradient problem <https://en.wikipedia.org/wiki/Vanishing_gradient_problem>
  * Rectifier <https://en.wikipedia.org/wiki/Rectifier_(neural_networks)>
  * Fine tuning <https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)>
  * Recurrent neural network <https://en.wikipedia.org/wiki/Recurrent_neural_network>
    * LSTM <https://en.wikipedia.org/wiki/Long_short-term_memory>

## ML tools

* scikit-learn <https://scikit-learn.org/stable>

## ML as a service

* Azure Machine Learning <https://azure.microsoft.com/en-us/products/machine-learning/>
* Amazon SageMaker <https://aws.amazon.com/sagemaker/>

## Deep learning frameworks

* TensorFlow <https://www.tensorflow.org/>
  * TFDS <https://www.tensorflow.org/datasets>
  * Keras <https://keras.io/>
* PyTorch <https://pytorch.org/>

## Programming language for AI

* Mojo <https://www.modular.com/mojo>

## Package management for computational science

* Anaconda distribution <http://anaconda.com/>

## Interactive computing

* JupyterLab / Jupiter Notebook <https://jupyter.org/>
* BeakerX <http://beakerx.com/>

## Typesetting system for scientific documents

* LaTeX <https://www.latex-project.org/>
  * TexLive <https://tug.org/texlive/>
* KaTeX <https://katex.org/>

## NLP

* n-gram <https://en.wikipedia.org/wiki/N-gram>
* tf-idf (term frequency‚Äìinverse document frequency) <https://en.wikipedia.org/wiki/Tf%E2%80%93idf>
* Word2vec <https://en.wikipedia.org/wiki/Word2vec>
  * fastText <https://fasttext.cc/>
* GloVe <https://nlp.stanford.edu/projects/glove/>
* ULMFiT <https://arxiv.org/abs/1801.06146>
* ELMo <https://arxiv.org/abs/1802.05365>
* Transformer <https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)>

## Pretrained LLM

* BERT <https://arxiv.org/abs/1810.04805>
* GPT-4 <https://openai.com/research/gpt-4> <https://arxiv.org/abs/2303.08774>
* LLaMA <https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/>
* InstructGPT <https://arxiv.org/abs/2203.02155>
* Codex <https://arxiv.org/abs/2107.03374>

## Coding assistant

* GitHub Copilot <https://github.com/features/copilot/>
* Visual Studio IntelliCode <https://marketplace.visualstudio.com/items?itemName=VisualStudioExptTeam.vscodeintellicode>
* CodeGPT <https://www.codegpt.co/>

----

## Timeline

### 1950s

üß† The Turing test, originally called the imitation game by Alan Turing in 1950, is a test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human.

üß† In machine learning, the perceptron (or McCulloch-Pitts neuron) is an algorithm for supervised learning of binary classifiers.
The first implementation was a machine built in 1958 at the Cornell Aeronautical Laboratory by Frank Rosenblatt.

üß† The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.

### 1970s

üõ¢Ô∏è The term "relational database" was first defined by E. F. Codd at IBM in 1970. Codd introduced the term in his research paper "A Relational Model of Data for Large Shared Data Banks".

üõ¢Ô∏è Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS).
First appeared: 1974

üõ¢Ô∏è Oracle Database (commonly referred to as Oracle DBMS, Oracle Autonomous Database, or simply as Oracle) is a proprietary multi-model database management system produced and marketed by Oracle Corporation.
Initial release: 1979

### 1980s

üß† In machine learning, backpropagation is a widely used algorithm for training feedforward artificial neural networks or other parameterized networks with differentiable nodes.
In 1986, David E. Rumelhart et al. published an experimental analysis of the technique. This contributed to the popularization of backpropagation and helped to initiate an active period of research in multilayer perceptrons.

üß† The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986.

üõ¢Ô∏è SQL was adopted as a standard by the ANSI in 1986 as SQL-86 and the ISO in 1987.

üõ¢Ô∏è Microsoft SQL Server is a proprietary relational database management system developed by Microsoft. As a database server, it is a software product with the primary function of storing and retrieving data as requested by other software applications.
Initial release: April 24, 1989

### 1990s

üß† In 1991, the autoencoder was first proposed as a nonlinear generalization of principal components analysis (PCA) by Kramer.

üß† R was started by professors Ross Ihaka and Robert Gentleman as a programming language to teach introductory statistics at the University of Auckland.
First appeared: August 1993

üõ¢Ô∏è MySQL is an open-source relational database management system (RDBMS).
MySQL is free and open-source software under the terms of the GNU General Public License, and is also available under a variety of proprietary licenses.
Initial release: 23 May 1995

üõ¢Ô∏è PostgreSQL also known as Postgres, is a free and open-source relational database management system (RDBMS) emphasizing extensibility and SQL compliance.
In 1996, the project was renamed to PostgreSQL to reflect its support for SQL.
Initial release: 8 July 1996

üß† A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes can create a cycle, allowing output from some nodes to affect subsequent input to the same nodes. This allows it to exhibit temporal dynamic behavior.
Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.

### 2000s

üõ¢Ô∏è SQLite is a database engine written in the C programming language. It is not a standalone app; rather, it is a library that software developers embed in their apps. As such, it belongs to the family of embedded databases.
Initial release: 17 August 2000

üß† Torch is an open-source machine learning library, a scientific computing framework, and a script language based on the Lua programming language.
Initial release: October 2002

üß† In 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU.
In 2005, another paper also emphasised the value of GPGPU for machine learning.

üß† In 2006, Geoffrey Hinton developed the deep belief network technique for training many-layered deep autoencoders.

üß† The scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau.
Initial release: June 2007

üõ¢Ô∏è Amazon Relational Database Service (or Amazon RDS) is a distributed relational database service by Amazon Web Services (AWS).
Amazon RDS was first released on 22 October 2009, supporting MySQL databases. This was followed by support for Oracle Database in June 2011, Microsoft SQL Server in May 2012, PostgreSQL in November 2013.

### 2010s

üß† Keras is an open-source software library that provides a Python interface for artificial neural networks. Keras acts as an interface for the TensorFlow library.
Initial release: 27 March 2015

üß† TensorFlow is a free and open-source software library for machine learning and artificial intelligence. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks.
Initial release: November 9, 2015

üß† PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing, originally developed by Meta AI and now part of the Linux Foundation umbrella.
Initial release: September 2016

üß† Amazon SageMaker is a cloud machine-learning platform that enables developers to create, train, and deploy machine-learning (ML) models in the cloud. It also enables developers to deploy ML models on embedded systems and edge-devices. SageMaker was launched in 29 November 2017.

üß† A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the fields of natural language processing (NLP) and computer vision (CV).
Transformers were introduced in 2017 by a team at Google Brain and are increasingly becoming the model of choice for NLP problems, replacing RNN models such as long short-term memory (LSTM).

üß† Generative Pre-trained Transformer 2 (GPT-2) is an open-source artificial intelligence created by OpenAI in February 2019.
GPT-2 translates text, answers questions, summarizes passages, and generates text output on a level that, while sometimes indistinguishable from that of humans, can become repetitive or nonsensical when generating long passages.

### 2020s

üß† Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model released in 2020 that uses deep learning to produce human-like text. Given an initial text as prompt, it will produce text that continues the prompt.

üß† LaMDA (Language Model for Dialogue Applications) is a family of conversational large language models developed by Google. Originally developed and introduced as Meena in 2020, the first-generation LaMDA was announced during the 2021 Google I/O keynote, while the second generation was announced the following year.

üß† GitHub Copilot is a cloud-based artificial intelligence tool developed by GitHub and OpenAI to assist users of Visual Studio Code, Visual Studio, Neovim, and JetBrains integrated development environments (IDEs) by autocompleting code.
On June 29, 2021, GitHub announced GitHub Copilot for technical preview in the Visual Studio Code development environment.

üß† OpenAI Codex is an artificial intelligence model developed by OpenAI. It parses natural language and generates code in response. It is used to power GitHub Copilot, a programming autocompletion tool developed for Visual Studio Code.
Codex is a descendant of OpenAI's GPT-3 model, fine-tuned for use in programming applications.
Released: Aug 10, 2021

üß† On May 11, 2022, Google unveiled LaMDA 2, the successor to LaMDA, during the 2022 Google I/O keynote.

üß† ChatGPT (Chat Generative Pre-trained Transformer) is an artificial-intelligence (AI) chatbot developed by OpenAI and launched in November 2022. It is built on top of OpenAI's GPT-3.5 and GPT-4 families of large language models (LLMs) and has been fine-tuned (an approach to transfer learning) using both supervised and reinforcement learning techniques.

üß† On February 7, 2023, Microsoft announced a major overhaul to Bing including the addition of chatbot functionality marketed as "the new Bing".

üß† Bard is a conversational artificial intelligence chatbot developed by Google, based on the LaMDA family of large language models. It was developed as a response to the rise of OpenAI's ChatGPT, and was released in a limited capacity in March 2023 to lukewarm responses.

üß† The original release of ChatGPT was based on GPT-3.5. A version based on GPT-4, the newest OpenAI model, was released on March 14, 2023, and is available for paid subscribers on a limited basis.
